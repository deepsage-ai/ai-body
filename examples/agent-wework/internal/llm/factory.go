package llm

import (
	"fmt"
	"os"
	"strings"

	"github.com/Ingenimax/agent-sdk-go/pkg/interfaces"
	"github.com/Ingenimax/agent-sdk-go/pkg/llm/anthropic"
	"github.com/Ingenimax/agent-sdk-go/pkg/llm/openai"
	"github.com/Ingenimax/agent-sdk-go/pkg/logging"

	"github.com/deepsage-ai/b0dy/examples/agent-wework/internal/config"
)

// CreateLLMFromConfig æ ¹æ®é…ç½®åˆ›å»ºLLMå®¢æˆ·ç«¯
func CreateLLMFromConfig(cfg *config.Config, logger logging.Logger) (interfaces.LLM, error) {
	// è·å–è¦ä½¿ç”¨çš„LLMåç§°
	llmName := cfg.LLM.Default

	// æ”¯æŒç¯å¢ƒå˜é‡è¦†ç›–
	if override := os.Getenv("LLM_PROVIDER"); override != "" {
		llmName = override
	}

	// æŸ¥æ‰¾å¯¹åº”çš„provideré…ç½®
	provider, ok := cfg.LLM.Providers[llmName]
	if !ok {
		return nil, fmt.Errorf("LLM provider '%s' not found in config", llmName)
	}

	// å¤„ç†ç¯å¢ƒå˜é‡å¼•ç”¨
	provider.APIKey = processEnvVar(provider.APIKey)
	provider.BaseURL = processEnvVar(provider.BaseURL)

	// å¦‚æœå¯ç”¨æ€è€ƒæ¨¡å¼ï¼Œè¾“å‡ºæç¤ºä¿¡æ¯
	if provider.ThinkingMode {
		fmt.Printf("ğŸ§  æ·±å…¥æ€è€ƒæ¨¡å¼å·²å¯ç”¨ (Provider: %s)\n", provider.Provider)
	}

	return createLLMClient(provider, logger)
}

// createLLMClient æ ¹æ®provideré…ç½®åˆ›å»ºå…·ä½“çš„LLMå®¢æˆ·ç«¯
func createLLMClient(config config.LLMProviderConfig, logger logging.Logger) (interfaces.LLM, error) {
	switch config.Provider {
	case "ollama":
		// Ollamaä½¿ç”¨OpenAIå…¼å®¹æ¥å£ï¼Œä¸éœ€è¦API Key
		client := openai.NewClient("",
			openai.WithBaseURL(config.BaseURL),
			openai.WithModel(config.Model),
			openai.WithLogger(logger),
			// æ·»åŠ æ¸©åº¦æ§åˆ¶ï¼Œè®©è¾“å‡ºæ›´ç¡®å®šæ€§
			openai.WithTemperature(0.3))

		// å¦‚æœå¯ç”¨æ€è€ƒæ¨¡å¼ï¼Œåˆ›å»ºæ”¯æŒreasoningçš„åŒ…è£…å™¨
		if config.ThinkingMode {
			reasoningLevel := config.ReasoningLevel
			if reasoningLevel == "" {
				reasoningLevel = "minimal" // é»˜è®¤ç®€æ´
			}
			fmt.Printf("âœ… Ollama æ€è€ƒæ¨¡å¼å·²å¯ç”¨ (æ¨ç†çº§åˆ«: %s)\n", reasoningLevel)
			return NewOpenAIThinkingWrapperWithLevel(client, reasoningLevel), nil
		}

		return client, nil

	case "qwen":
		// åƒé—®ä½¿ç”¨DashScopeçš„OpenAIå…¼å®¹æ¥å£
		if config.APIKey == "" {
			return nil, fmt.Errorf("qwen requires API key")
		}

		client := openai.NewClient(config.APIKey,
			openai.WithBaseURL(config.BaseURL),
			openai.WithModel(config.Model),
			openai.WithLogger(logger))

		// å¦‚æœå¯ç”¨æ€è€ƒæ¨¡å¼ï¼Œåˆ›å»ºæ”¯æŒreasoningçš„åŒ…è£…å™¨
		if config.ThinkingMode {
			reasoningLevel := config.ReasoningLevel
			if reasoningLevel == "" {
				reasoningLevel = "minimal" // é»˜è®¤ç®€æ´
			}
			fmt.Printf("âœ… åƒé—® æ€è€ƒæ¨¡å¼å·²å¯ç”¨ (æ¨ç†çº§åˆ«: %s)\n", reasoningLevel)
			return NewOpenAIThinkingWrapperWithLevel(client, reasoningLevel), nil
		}

		return client, nil

	case "openai":
		// æ ‡å‡†OpenAI
		if config.APIKey == "" {
			return nil, fmt.Errorf("openai requires API key")
		}

		var client interfaces.LLM
		if config.BaseURL != "" {
			client = openai.NewClient(config.APIKey,
				openai.WithBaseURL(config.BaseURL),
				openai.WithModel(config.Model),
				openai.WithLogger(logger))
		} else {
			client = openai.NewClient(config.APIKey,
				openai.WithModel(config.Model),
				openai.WithLogger(logger))
		}

		// å¦‚æœå¯ç”¨æ€è€ƒæ¨¡å¼ï¼Œåˆ›å»ºæ”¯æŒreasoningçš„åŒ…è£…å™¨
		// å¯¹äºo1ç³»åˆ—æ¨¡å‹ï¼Œè¿™ä¼šè‡ªåŠ¨å¯ç”¨å†…éƒ¨æ¨ç†
		if config.ThinkingMode {
			reasoningLevel := config.ReasoningLevel
			if reasoningLevel == "" {
				reasoningLevel = "minimal" // é»˜è®¤ç®€æ´
			}
			fmt.Printf("âœ… OpenAI æ€è€ƒæ¨¡å¼å·²å¯ç”¨ (æ¨¡å‹: %s, æ¨ç†çº§åˆ«: %s)\n", config.Model, reasoningLevel)
			return NewOpenAIThinkingWrapperWithLevel(client, reasoningLevel), nil
		}

		return client, nil

	case "claude":
		// Claudeä½¿ç”¨ä¸“é—¨çš„anthropicåŒ…
		if config.APIKey == "" {
			return nil, fmt.Errorf("claude requires API key")
		}

		// åˆ›å»ºåŸºç¡€å®¢æˆ·ç«¯
		client := anthropic.NewClient(config.APIKey,
			anthropic.WithModel(config.Model),
			anthropic.WithLogger(logger))

		// æ£€æŸ¥æ˜¯å¦æ”¯æŒthinking mode
		if config.ThinkingMode && anthropic.SupportsThinking(config.Model) {
			fmt.Printf("âœ… æ¨¡å‹ %s æ”¯æŒæ·±å…¥æ€è€ƒæ¨¡å¼\n", config.Model)
			// åˆ›å»ºåŒ…è£…å®¢æˆ·ç«¯ä»¥å¯ç”¨thinking
			return NewThinkingLLMWrapper(client, config.Model), nil
		} else if config.ThinkingMode {
			fmt.Printf("âš ï¸  è­¦å‘Š: æ¨¡å‹ %s ä¸æ”¯æŒæ·±å…¥æ€è€ƒæ¨¡å¼\n", config.Model)
		}

		return client, nil

	case "custom":
		// è‡ªå®šä¹‰OpenAIå…¼å®¹ç«¯ç‚¹
		if config.BaseURL == "" {
			return nil, fmt.Errorf("custom provider requires base_url")
		}
		return openai.NewClient(config.APIKey,
			openai.WithBaseURL(config.BaseURL),
			openai.WithModel(config.Model),
			openai.WithLogger(logger)), nil

	default:
		return nil, fmt.Errorf("unsupported LLM provider: %s", config.Provider)
	}
}

// processEnvVar å¤„ç†ç¯å¢ƒå˜é‡å¼•ç”¨
func processEnvVar(value string) string {
	if strings.HasPrefix(value, "${") && strings.HasSuffix(value, "}") {
		envVar := strings.Trim(value, "${}")
		return os.Getenv(envVar)
	}
	return value
}
